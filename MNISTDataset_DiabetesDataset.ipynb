{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset: Preparation, PCA, Clasfification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning - the source of the MNIST - library has changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Split into training and test\n",
    "# test_size: what proportion of original data is used for test set\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split( mnist.data, mnist.target, test_size=1/7.0, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape #28x28 - images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(train_img) # Scaling\n",
    "# Apply transform to both the training set and the test set.\n",
    "train_img = scaler.transform(train_img) # Scaling\n",
    "test_img = scaler.transform(test_img) # Scaling\n",
    "copy_test_img = test_img # is saved for display, see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img[0].shape # digit '4' at the 0-th position in the training data set as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Representation of the digit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "first_array=train_img[0].reshape(28,28)\n",
    "plt.imshow(first_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=0.95)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.95)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=0.95)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(n_components=0.95) # The variance of the data must be more than 95%.\n",
    "pca.fit(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = pca.transform(train_img)\n",
    "test_img = pca.transform(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 327)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')  \n",
    "logisticRegr.fit(train_img, train_lbl) # Logistic Regression - Model training (Labels start by 0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict for One Observation (image)\n",
    "logisticRegr.predict(test_img[10].reshape(1,-1)) # transform in a matrix 1x330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 327)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img[10].reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "first_array=copy_test_img[10].reshape(28,28)\n",
    "plt.imshow(first_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '4', '1', '2', '4', '7', '7', '1', '1', '7'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict for 10 images\n",
    "logisticRegr.predict(test_img[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '4', '1', ..., '1', '3', '0'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict for all in test\n",
    "logisticRegr.predict(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9201"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.score(test_img, test_lbl) #accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network for MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( mnist.data, mnist.target, test_size=1/7.0, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7', '3', '0', '1', '2', ..., '7', '8', '7', '1', '1']\n",
       "Length: 60000\n",
       "Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "### One hot encoding\n",
    "Y_train_arr = Y_train.array.reshape(-1,1)\n",
    "Y_test_arr = Y_test.array.reshape(-1,1)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_Y_train = onehot_encoder.fit_transform(Y_train_arr)\n",
    "onehot_Y_test = onehot_encoder.fit_transform(Y_test_arr)\n",
    "\n",
    "print(onehot_Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4238 - accuracy: 0.2512\n",
      "Epoch 2/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.6556 - accuracy: 0.3076\n",
      "Epoch 3/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1.4801 - accuracy: 0.3734\n",
      "Epoch 4/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1.2486 - accuracy: 0.4872\n",
      "Epoch 5/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1.1114 - accuracy: 0.5566\n",
      "Epoch 6/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.0268 - accuracy: 0.5947\n",
      "Epoch 7/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.9896 - accuracy: 0.6064\n",
      "Epoch 8/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9457 - accuracy: 0.6244\n",
      "Epoch 9/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8251 - accuracy: 0.6952\n",
      "Epoch 10/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.6953 - accuracy: 0.7702\n",
      "Epoch 11/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.6057 - accuracy: 0.8205\n",
      "Epoch 12/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.5342 - accuracy: 0.8547\n",
      "Epoch 13/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.4850 - accuracy: 0.8681\n",
      "Epoch 14/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.4438 - accuracy: 0.8820\n",
      "Epoch 15/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4179 - accuracy: 0.8874\n",
      "Epoch 16/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3927 - accuracy: 0.8946\n",
      "Epoch 17/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3792 - accuracy: 0.8976\n",
      "Epoch 18/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3594 - accuracy: 0.9026\n",
      "Epoch 19/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3456 - accuracy: 0.9074\n",
      "Epoch 20/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.3340 - accuracy: 0.9077\n",
      "Epoch 21/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3177 - accuracy: 0.9122\n",
      "Epoch 22/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3121 - accuracy: 0.9144\n",
      "Epoch 23/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3018 - accuracy: 0.9157\n",
      "Epoch 24/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2896 - accuracy: 0.9194\n",
      "Epoch 25/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2854 - accuracy: 0.9199\n",
      "Epoch 26/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2762 - accuracy: 0.9221\n",
      "Epoch 27/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2714 - accuracy: 0.9248\n",
      "Epoch 28/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2654 - accuracy: 0.9242\n",
      "Epoch 29/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2606 - accuracy: 0.9258\n",
      "Epoch 30/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2560 - accuracy: 0.9261\n",
      "Epoch 31/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2544 - accuracy: 0.9265\n",
      "Epoch 32/50\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.2457 - accuracy: 0.9292\n",
      "Epoch 33/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2430 - accuracy: 0.9309\n",
      "Epoch 34/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2373 - accuracy: 0.9325\n",
      "Epoch 35/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2362 - accuracy: 0.9319\n",
      "Epoch 36/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2357 - accuracy: 0.9317\n",
      "Epoch 37/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2325 - accuracy: 0.9329\n",
      "Epoch 38/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2282 - accuracy: 0.9336\n",
      "Epoch 39/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2274 - accuracy: 0.9347\n",
      "Epoch 40/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2239 - accuracy: 0.9353\n",
      "Epoch 41/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2243 - accuracy: 0.9345\n",
      "Epoch 42/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2188 - accuracy: 0.9373\n",
      "Epoch 43/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2216 - accuracy: 0.9367\n",
      "Epoch 44/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2146 - accuracy: 0.9379\n",
      "Epoch 45/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2136 - accuracy: 0.9385\n",
      "Epoch 46/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2130 - accuracy: 0.9377\n",
      "Epoch 47/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2115 - accuracy: 0.9386\n",
      "Epoch 48/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2091 - accuracy: 0.9394\n",
      "Epoch 49/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2055 - accuracy: 0.9408\n",
      "Epoch 50/50\n",
      "600/600 [==============================] - 2s 2ms/step - loss: 0.2040 - accuracy: 0.9404\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=784, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train,onehot_Y_train, epochs=50, batch_size=100)\n",
    "accuracy = model.evaluate(X_test, onehot_Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.315684050321579, 0.9230999946594238]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification on PCA reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "PCA_X_train, PCA_X_test, PCA_Y_train, PCA_Y_test = train_test_split( mnist.data, mnist.target, test_size=1/7.0, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(PCA_X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "PCA_X_train = scaler.transform(PCA_X_train)\n",
    "PCA_X_test = scaler.transform(PCA_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(.95)\n",
    "pca.fit(PCA_X_train)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_X_train = pca.transform(PCA_X_train)\n",
    "PCA_X_test = pca.transform(PCA_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 327)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCA_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCA_Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "600/600 [==============================] - 2s 2ms/step - loss: 1.1498 - accuracy: 0.6483\n",
      "Epoch 2/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.3654 - accuracy: 0.9007\n",
      "Epoch 3/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2845 - accuracy: 0.9198\n",
      "Epoch 4/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2531 - accuracy: 0.9282\n",
      "Epoch 5/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2353 - accuracy: 0.9320\n",
      "Epoch 6/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2210 - accuracy: 0.9358\n",
      "Epoch 7/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2113 - accuracy: 0.9384\n",
      "Epoch 8/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2036 - accuracy: 0.9406\n",
      "Epoch 9/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1969 - accuracy: 0.9431\n",
      "Epoch 10/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1926 - accuracy: 0.9444\n",
      "Epoch 11/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1876 - accuracy: 0.9458\n",
      "Epoch 12/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1831 - accuracy: 0.9471\n",
      "Epoch 13/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1802 - accuracy: 0.9481\n",
      "Epoch 14/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.9494\n",
      "Epoch 15/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1743 - accuracy: 0.9493\n",
      "Epoch 16/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1714 - accuracy: 0.9499\n",
      "Epoch 17/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1678 - accuracy: 0.9506\n",
      "Epoch 18/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1669 - accuracy: 0.9510\n",
      "Epoch 19/50\n",
      "600/600 [==============================] - 2s 2ms/step - loss: 0.1647 - accuracy: 0.9512\n",
      "Epoch 20/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1624 - accuracy: 0.9523\n",
      "Epoch 21/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1611 - accuracy: 0.9528\n",
      "Epoch 22/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1586 - accuracy: 0.9534\n",
      "Epoch 23/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1574 - accuracy: 0.9538\n",
      "Epoch 24/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1559 - accuracy: 0.9536\n",
      "Epoch 25/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1553 - accuracy: 0.9548\n",
      "Epoch 26/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1548 - accuracy: 0.9543\n",
      "Epoch 27/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1529 - accuracy: 0.9545\n",
      "Epoch 28/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1511 - accuracy: 0.9552\n",
      "Epoch 29/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1507 - accuracy: 0.9547\n",
      "Epoch 30/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1491 - accuracy: 0.9553\n",
      "Epoch 31/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1481 - accuracy: 0.9557\n",
      "Epoch 32/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1480 - accuracy: 0.9557\n",
      "Epoch 33/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1476 - accuracy: 0.9556\n",
      "Epoch 34/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1454 - accuracy: 0.9567\n",
      "Epoch 35/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1456 - accuracy: 0.9566\n",
      "Epoch 36/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1452 - accuracy: 0.9573\n",
      "Epoch 37/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1445 - accuracy: 0.9570\n",
      "Epoch 38/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1439 - accuracy: 0.9567\n",
      "Epoch 39/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.1430 - accuracy: 0.9571\n",
      "Epoch 40/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1423 - accuracy: 0.9578\n",
      "Epoch 41/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1409 - accuracy: 0.9575\n",
      "Epoch 42/50\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.1417 - accuracy: 0.9576\n",
      "Epoch 43/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1410 - accuracy: 0.9574\n",
      "Epoch 44/50\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.1401 - accuracy: 0.9583\n",
      "Epoch 45/50\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.1387 - accuracy: 0.9582\n",
      "Epoch 46/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1388 - accuracy: 0.9586\n",
      "Epoch 47/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1384 - accuracy: 0.9586\n",
      "Epoch 48/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1374 - accuracy: 0.9587\n",
      "Epoch 49/50\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.1367 - accuracy: 0.9582\n",
      "Epoch 50/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1362 - accuracy: 0.9588\n",
      "[0.3320973813533783, 0.9390000104904175]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=327, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(PCA_X_train,onehot_Y_train, epochs=50, batch_size=100)\n",
    "accuracy = model.evaluate(PCA_X_test, onehot_Y_test, verbose=0)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Diabetes Dataset contains data on various health factors of individuals and also the information whether or not they have diabietes. \n",
    "\n",
    "## In the following this dataset will be cleaned. Then Neural Network will be trained to predict whether a person has diabetes given some information of their medical history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.00000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness     Insulin   BMI  \\\n",
       "0              6    148.0           72.0       35.00000  155.548223  33.6   \n",
       "1              1     85.0           66.0       29.00000  155.548223  26.6   \n",
       "2              8    183.0           64.0       29.15342  155.548223  23.3   \n",
       "3              1     89.0           66.0       23.00000   94.000000  28.1   \n",
       "4              0    137.0           40.0       35.00000  168.000000  43.1   \n",
       "..           ...      ...            ...            ...         ...   ...   \n",
       "763           10    101.0           76.0       48.00000  180.000000  32.9   \n",
       "764            2    122.0           70.0       27.00000  155.548223  36.8   \n",
       "765            5    121.0           72.0       23.00000  112.000000  26.2   \n",
       "766            1    126.0           60.0       29.15342  155.548223  30.1   \n",
       "767            1     93.0           70.0       31.00000  155.548223  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/diabetes.csv\")\n",
    "\n",
    "df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']].replace(0,np.NaN)\n",
    "df.fillna(df.mean(), inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[[\"Glucose\",'BMI','Age','DiabetesPedigreeFunction']]\n",
    "labels = df.Outcome\n",
    "features_train,features_test,labels_train,labels_test = train_test_split(features,labels,stratify=df.Outcome,test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153    0\n",
      "557    0\n",
      "188    1\n",
      "522    0\n",
      "182    0\n",
      "      ..\n",
      "579    1\n",
      "52     0\n",
      "546    1\n",
      "48     1\n",
      "391    1\n",
      "Name: Outcome, Length: 460, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "92/92 [==============================] - 1s 2ms/step - loss: 1.5772 - accuracy: 0.5043\n",
      "Epoch 2/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6955 - accuracy: 0.4739\n",
      "Epoch 3/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6864 - accuracy: 0.6478\n",
      "Epoch 4/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6790 - accuracy: 0.6500\n",
      "Epoch 5/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6723 - accuracy: 0.6500\n",
      "Epoch 6/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6644 - accuracy: 0.6500\n",
      "Epoch 7/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6713 - accuracy: 0.6500\n",
      "Epoch 8/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6626 - accuracy: 0.6500\n",
      "Epoch 9/150\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6500\n",
      "Epoch 10/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6574 - accuracy: 0.6500\n",
      "Epoch 11/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6554 - accuracy: 0.6500\n",
      "Epoch 12/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6542 - accuracy: 0.6500\n",
      "Epoch 13/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6523 - accuracy: 0.6500\n",
      "Epoch 14/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.6500\n",
      "Epoch 15/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6498 - accuracy: 0.6500\n",
      "Epoch 16/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6495 - accuracy: 0.6500\n",
      "Epoch 17/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.6500\n",
      "Epoch 18/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6484 - accuracy: 0.6500\n",
      "Epoch 19/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6476 - accuracy: 0.6500\n",
      "Epoch 20/150\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6473 - accuracy: 0.6500\n",
      "Epoch 21/150\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6468 - accuracy: 0.6500\n",
      "Epoch 22/150\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6468 - accuracy: 0.6500\n",
      "Epoch 23/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6465 - accuracy: 0.6500\n",
      "Epoch 24/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6461 - accuracy: 0.6500\n",
      "Epoch 25/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6456 - accuracy: 0.6500\n",
      "Epoch 26/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6474 - accuracy: 0.6500\n",
      "Epoch 27/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6456 - accuracy: 0.6500\n",
      "Epoch 28/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6449 - accuracy: 0.6500\n",
      "Epoch 29/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6453 - accuracy: 0.6500\n",
      "Epoch 30/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6446 - accuracy: 0.6500\n",
      "Epoch 31/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6449 - accuracy: 0.6500\n",
      "Epoch 32/150\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6448 - accuracy: 0.6500\n",
      "Epoch 33/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6444 - accuracy: 0.6500\n",
      "Epoch 34/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6450 - accuracy: 0.6500\n",
      "Epoch 35/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.6500\n",
      "Epoch 36/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.6500\n",
      "Epoch 37/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6449 - accuracy: 0.6500\n",
      "Epoch 38/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6445 - accuracy: 0.6500\n",
      "Epoch 39/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6441 - accuracy: 0.6500\n",
      "Epoch 40/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6447 - accuracy: 0.6500\n",
      "Epoch 41/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.6500\n",
      "Epoch 42/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6442 - accuracy: 0.6500\n",
      "Epoch 43/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.6500\n",
      "Epoch 44/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6453 - accuracy: 0.6500\n",
      "Epoch 45/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.6500\n",
      "Epoch 46/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.6500\n",
      "Epoch 47/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6445 - accuracy: 0.6500\n",
      "Epoch 48/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.6500\n",
      "Epoch 49/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6441 - accuracy: 0.6500\n",
      "Epoch 50/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.6500\n",
      "Epoch 51/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.6500\n",
      "Epoch 52/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.6500\n",
      "Epoch 53/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.6500\n",
      "Epoch 54/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.6500\n",
      "Epoch 55/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.6500\n",
      "Epoch 56/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.6500\n",
      "Epoch 57/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.6500\n",
      "Epoch 58/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6441 - accuracy: 0.6500\n",
      "Epoch 59/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6455 - accuracy: 0.6500\n",
      "Epoch 60/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6446 - accuracy: 0.6500\n",
      "Epoch 61/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.6500\n",
      "Epoch 62/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 63/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 64/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.6500\n",
      "Epoch 65/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6451 - accuracy: 0.6500\n",
      "Epoch 66/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.6500\n",
      "Epoch 67/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6440 - accuracy: 0.6500\n",
      "Epoch 68/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 69/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 70/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 71/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 72/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6491 - accuracy: 0.6500\n",
      "Epoch 73/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6472 - accuracy: 0.6500\n",
      "Epoch 74/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6440 - accuracy: 0.6500\n",
      "Epoch 75/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.6500\n",
      "Epoch 76/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 77/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.6500\n",
      "Epoch 78/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 79/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 80/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 81/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.6500\n",
      "Epoch 82/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6462 - accuracy: 0.6500\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.6500\n",
      "Epoch 84/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.6500\n",
      "Epoch 85/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 86/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 87/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 88/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 89/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 90/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 91/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.6500\n",
      "Epoch 92/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.6500\n",
      "Epoch 93/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 94/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.6500\n",
      "Epoch 95/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6459 - accuracy: 0.6500\n",
      "Epoch 96/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6445 - accuracy: 0.6500\n",
      "Epoch 97/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 98/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 99/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 100/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 101/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 102/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 103/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 104/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 105/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 106/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 107/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 108/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 109/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.6500\n",
      "Epoch 110/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 111/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 112/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 113/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6450 - accuracy: 0.6500\n",
      "Epoch 114/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6450 - accuracy: 0.6500\n",
      "Epoch 115/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.6500\n",
      "Epoch 116/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6441 - accuracy: 0.6500\n",
      "Epoch 117/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6442 - accuracy: 0.6500\n",
      "Epoch 118/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6441 - accuracy: 0.6500\n",
      "Epoch 119/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 120/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 121/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 122/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 123/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 124/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 125/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 126/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 127/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 128/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 129/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.6500\n",
      "Epoch 130/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 131/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 132/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 133/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 134/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 135/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 136/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 137/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 138/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 139/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 140/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 141/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 142/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 143/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 144/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 145/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 146/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 147/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 148/150\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "Epoch 149/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6500\n",
      "Epoch 150/150\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6500\n",
      "[0.6386576890945435, 0.6525974273681641]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=4, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(features_train,labels_train, epochs=150, batch_size=5)\n",
    "accuracy = model.evaluate(features_test, labels_test, verbose=0)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
